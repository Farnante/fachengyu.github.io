<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Facheng Yu</title>
  
  <meta name="author" content="Facheng Yu">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/icon.png">
</head>

<body>
    <table style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <h2>About this website</h2>
        <p>Here are <a href="https://fachengyu.github.io/">Facheng Yu</a>'s notes while learning <em><a href="https://web.stanford.edu/class/cs229t/notes.pdf">CS229T/STAT231: Statistical Learning Theory (Winter 2016)</a></em>.
        </p>
        </p>
        <p> Last update: 9/6/2022
        </p>
      </td>
    </tr>
  </table>
 
  <table style="width:100%;max-width:900px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        
        <h2>Content</h2>
        <ul> 
          <li><b>Chapter 1: Overview</b></li>
            <ul>
                <li>Section 1.1-1.5: <a href="SLnotes/1-1.pdf">Overview of Asymptotics, Uniform Convergence, Kernel Methods and Online Learning</a>.</li>
            </ul>
          </br>
        
          <li><b>Chapter 2: Asymptotics</b></li>
            <ul>
                <li>Section 2.1-2.4: <a href="SLnotes/2-1.pdf">Guassian Mean Estimation, Multinomial Estimation and Exponential Families</a>.</li>
                <li>Section 2.5: <a href="SLnotes/2-5.pdf">Maximum Entropy Principle</a>.</li>
                <li>Section 2.6: <a href="SLnotes/2-6.pdf">Method of Moments for Latent-Variable Models</a>.</li>
                <li>Section 2.7: <a href="SLnotes/2-7.pdf">Fixed Design Linear Regression</a>.</li>
                <li>Section 2.8: <a href="SLnotes/2-8.pdf">General Loss Functions and Random Design</a>.</li>
                <li>Section 2.9: <a href="SLnotes/2-9.pdf">Regularized Fixed Design Linear Regression</a>.</li>
            </ul>
          </br>
    
          <li><b>Chapter 3:Uniform Convergence</b></li>
            <ul>
                <li>Section 3.1-3.2: <a href="SLnotes/3-1.pdf">Overview and Formal Setup</a>.</li>
                <li>Section 3.3: <a href="SLnotes/3-3.pdf">Realizable Finite Hypothesis Classes</a>.</li>
                <li>Section 3.4-3.5: <a href="SLnotes/3-4.pdf">Generalization Bounds and Hoeffding's Inequality</a>.</li>
                <li>Section 3.6: <a href="SLnotes/3-6.pdf">Finite Hypothesis Classes</a>.</li>
                <li>Section 3.7: <a href="SLnotes/3-7.pdf">McDiarmid's Inequality</a>.</li>
                <li>Section 3.8: <a href="SLnotes/3-8.pdf">Rademacher Complexity</a>.</li>
                <li>Section 3.9-3.10: <a href="SLnotes/3-9.pdf">Massart's Finite Lemma and Shattering Coefficient</a>.</li>
                <li>Section 3.11: <a href="SLnotes/3-11.pdf">VC Dimension</a>.</li>
                <li>Section 3.12: <a href="SLnotes/3-12.pdf">Norm-Constrained Hypothesis Classes</a>.</li>
                <li>Section 3.13: <a href="SLnotes/3-13.pdf">Covering Number (Metric Entropy)</a>.</li>
                <li>Section 3.14: <a href="SLnotes/3-14.pdf">Algorithmic Stability</a>.</li>
                <li>Section 3.15: <a href="SLnotes/3-15.pdf">PAC-Bayesian Bounds</a>.</li>
                <li>Section 3.16: <a href="SLnotes/3-16.pdf">Summary of Chapter 3</a>.</li>
            </ul>
          </br>

          <li><b>Chapter 5:Online learning</b></li>
            <ul>
                <li>Section 5.1-5.3: <a href="SLnotes/5-1.pdf">Online Learning and Online Convex Optimization</a>.</li>
                <li>Section 5.4: <a href="SLnotes/5-4.pdf">Follow The Leader (FTL)</a>.</li>
                <li>Section 5.5: <a href="SLnotes/5-5.pdf">Follow The Regularized Leader (FTRL)</a>.</li>
                <li>Section 5.6: <a href="SLnotes/5-6.pdf">Online Subgradient Descent (OGD)</a>.</li>
                <li>Section 5.7: <a href="SLnotes/5-7.pdf">Online Mirror Descent (OMD)</a>.</li>
                <li>Section 5.8: <a href="SLnotes/5-8.pdf">Regret Bounds with Bergman Divergences</a>.</li>
                <li>Section 5.9: <a href="SLnotes/5-9.pdf">Strong Convexity and Smoothness</a>.</li>
            </ul>
          </br>

        </ul>
        
      </td>
    </tr>
  </table>

</body>

</html>
